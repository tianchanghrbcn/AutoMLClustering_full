@InProceedings{10.1007/978-3-031-72506-731,
author="Jabiyeva, Aynur J.",
editor="Aliev, Rafik A.
and Kacprzyk, Janusz
and Pedrycz, Witold
and Jamshidi, Mo.
and Babanli, M.B.
and Sadikoglu, Fahreddin M.",
title="State of the Art of Big Data Analytics and Clustering Algorithms in Biomedicine",
booktitle="16th International Conference on Applications of Fuzzy Systems, Soft Computing and Artificial Intelligence Tools -- ICAFS-2023",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="228--234",
abstract="Currently, big data has become the main driving force of the information society. It could change healthcare. The article by big data in healthcare are also analyzed. Personal medical data security issues are reviewed, recommendations are made for the development of the healthcare sector. The concept of Big Data refers not only to the size of data, but also to the process of creating value from it. Big Data can really improve the healthcare system, choose the appropriate treatment method and reduce other healthcare problems. Big Data now covers all areas of human life, including biological and medical. Big Data is not only a reality for biomedical professionals, but also an effective tool for understanding and searching for new knowledge. Clustering, cluster analysis is defined as an unsupervised learning approach for analyzing data into groups, usually grouping similar objects into classes called clusters. Cluster-based techniques attempt to learn and estimate these parameters from the supplied data. A global dataset of patients with different diseases and experiences will help. By processing this data, we can create databases with different samples so that the patient's treatment plan is as appropriate and accurate as possible. This approach is already used in other areas of medicine, for example, for the recognition of X-ray images. Using fuzzy logic, machine learning and artificial intelligence algorithms, data from databases can be used to generate predefined patient treatment patterns. This can help determine the diagnosis and prescribe the right treatment in cases where doctors take a long time to decide on the right approach.",
isbn="978-3-031-72506-7"
}

@article{Aljohani2024,
  author = {Abeer Aljohani},
  title = {Optimizing Patient Stratification in Healthcare: A Comparative Analysis of Clustering Algorithms for EHR Data},
  journal = {International Journal of Computational Intelligence Systems},
  volume = {17},
  number = {1},
  pages = {173},
  year = {2024},
  month = {July},
  doi = {10.1007/s44196-024-00568-8},
  url = {https://doi.org/10.1007/s44196-024-00568-8},
  issn = {1875-6883},
  publisher = {Springer}
}

@INPROCEEDINGS{10729915,
  author={Leevy, Joffrey L. and Salekshahrezaee, Zahra and Khoshgoftaar, Taghi M.},
  booktitle={2024 IEEE 10th International Conference on Big Data Computing Service and Machine Learning Applications (BigDataService)}, 
  title={A Review of Unsupervised Anomaly Detection Techniques for Health Insurance Fraud}, 
  year={2024},
  volume={},
  number={},
  pages={141-149},
  keywords={Surveys;Incremental learning;Reviews;Transfer learning;Insurance;Medical services;Forestry;Real-time systems;Fraud;Anomaly detection;anomaly detection;health insurance fraud;unsupervised learning;machine learning;big data analytics},
  doi={10.1109/BigDataService62917.2024.00028}}

@article{Passlick2021,
  author = {Jens Passlick and Sonja Dreyer and Daniel Olivotti and Lukas Grützner and Dennis Eilers and Michael H. Breitner},
  title = {Predictive maintenance as an internet of things enabled business model: A taxonomy},
  journal = {Electronic Markets},
  volume = {31},
  number = {1},
  pages = {67--87},
  year = {2021},
  month = {March},
  doi = {10.1007/s12525-020-00440-5},
  url = {https://doi.org/10.1007/s12525-020-00440-5},
  issn = {1422-8890},
  publisher = {Springer}
}

@article{Abdulhameed2024,
  author = {Zaki Abdulhameed, Tiba and Yousif, Suhad A. and Samawi, Venus W. and Imad Al-Shaikhli, Hasnaa},
  journal = {IEEE Access}, 
  title = {SS-DBSCAN: Semi-Supervised Density-Based Spatial Clustering of Applications With Noise for Meaningful Clustering in Diverse Density Data}, 
  year = {2024},
  volume = {12},
  pages = {131507-131520},
  keywords = {Clustering algorithms, Noise measurement, Measurement, Complexity theory, Classification algorithms, Wireless sensor networks, Semisupervised learning, Unsupervised learning, Text categorization, Clustering, DBSCAN, Semi-supervised clustering, Unsupervised classification, Word classification},
  doi = {10.1109/ACCESS.2024.3457587}
}

@article{Bandyapadhyay2024,
  author = {Sayan Bandyapadhyay and Zachary Friggstad and Ramin Mousavi},
  title = {Parameterized Approximation Algorithms and Lower Bounds for k-Center Clustering and Variants},
  journal = {Algorithmica},
  volume = {86},
  number = {8},
  pages = {2557--2574},
  year = {2024},
  month = {August},
  doi = {10.1007/s00453-024-01236-1},
  url = {https://doi.org/10.1007/s00453-024-01236-1},
  issn = {1432-0541}
}

@article{Zhang2025,
  title = {The distance and entropy measures-based intuitionistic fuzzy C-means and similarity matrix clustering algorithms and their applications},
  journal = {Applied Soft Computing},
  volume = {169},
  pages = {112581},
  year = {2025},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2024.112581},
  url = {https://www.sciencedirect.com/science/article/pii/S1568494624013553},
  author = {Yueyue Zhang and Han-Liang Huang},
  keywords = {Mixed correlation coefficient, Distance measures, Entropy, Dynamic time warping, Clustering algorithms}
}

@article{Lin2024,
  title = {Data-driven solutions: Uncovering the hidden potential of big data technologies in building low-carbon cities},
  journal = {Computers \& Industrial Engineering},
  volume = {197},
  pages = {110543},
  year = {2024},
  issn = {0360-8352},
  doi = {10.1016/j.cie.2024.110543},
  url = {https://www.sciencedirect.com/science/article/pii/S0360835224006648},
  author = {Zihao Lin},
  keywords = {National Big Data Comprehensive Pilot Zone, Carbon emission, Green innovation, Internet}
}

@article{Atif2024,
  author = {M. Atif and M. Farooq and M. Shafiq and others},
  title = {Uncovering the impact of outliers on clusters’ evolution in temporal data-sets: an empirical analysis},
  journal = {Scientific Reports},
  volume = {14},
  pages = {30674},
  year = {2024},
  doi = {10.1038/s41598-024-75928-7},
  url = {https://doi.org/10.1038/s41598-024-75928-7}
}

@article{Guo2024,
  author    = {Guo, H. and Yin, H. and Song, S. and others},
  title     = {Application of density clustering with noise combined with particle swarm optimization in UWB indoor positioning},
  journal   = {Scientific Reports},
  volume    = {14},
  pages     = {13121},
  year      = {2024},
  doi       = {10.1038/s41598-024-63358-4},
  url       = {https://doi.org/10.1038/s41598-024-63358-4}
}

@article{Ni2023,
author = {Ni, Wei and Miao, Xiaoye and Zhao, Xiangyu and Wu, Yangyang and Liang, Shuwei and Yin, Jianwei},
title = {Automatic Data Repair: Are We Ready to Deploy?},
year = {2024},
issue_date = {June 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/3675034.3675051},
doi = {10.14778/3675034.3675051},
abstract = {Data quality is paramount in today's data-driven world, especially in the era of generative AI. Dirty data with errors and inconsistencies usually leads to flawed insights, unreliable decision-making, and biased or low-quality outputs from generative models. The study of repairing erroneous data has gained significant importance. Existing data repair algorithms differ in information utilization, problem settings, and are tested in limited scenarios. In this paper, we compare and summarize these algorithms with a driven information-based taxonomy. We systematically conduct a comprehensive evaluation of 12 mainstream data repair algorithms on 12 datasets under the settings of various data error rates, error types, and 4 downstream analysis tasks, assessing their error reduction performance with a novel but practical metric. We develop an effective and unified repair optimization strategy that substantially benefits the state of the arts. We conclude that, it is always worthy of data repair. The clean data does not determine the upper bound of data analysis performance. We provide valuable guidelines, challenges, and promising directions in the data repair domain. We anticipate this paper enabling researchers and users to well understand and deploy data repair algorithms in practice.},
journal = {Proc. VLDB Endow.},
month = jun,
pages = {2617–2630},
numpages = {14}
}

@INPROCEEDINGS{9458702,
  author={Li, Peng and Rao, Xi and Blase, Jennifer and Zhang, Yue and Chu, Xu and Zhang, Ce},
  booktitle={2021 IEEE 37th International Conference on Data Engineering (ICDE)}, 
  title={CleanML: A Study for Evaluating the Impact of Data Cleaning on ML Classification Tasks}, 
  year={2021},
  volume={},
  number={},
  pages={13-24},
  keywords={Training;Systematics;Machine learning algorithms;Machine learning;Cleaning;Data models;Classification algorithms;Data Cleaning;Machine Learning},
  doi={10.1109/ICDE51399.2021.00009}}

@article{Rekatsinas2017,
author = {Rekatsinas, Theodoros and Chu, Xu and Ilyas, Ihab F. and R\'{e}, Christopher},
title = {HoloClean: holistic data repairs with probabilistic inference},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137628.3137631},
doi = {10.14778/3137628.3137631},
abstract = {We introduce HoloClean, a framework for holistic data repairing driven by probabilistic inference. HoloClean unifies qualitative data repairing, which relies on integrity constraints or external data sources, with quantitative data repairing methods, which leverage statistical properties of the input data. Given an inconsistent dataset as input, HoloClean automatically generates a probabilistic program that performs data repairing. Inspired by recent theoretical advances in probabilistic inference, we introduce a series of optimizations which ensure that inference over HoloClean's probabilistic model scales to instances with millions of tuples. We show that HoloClean finds data repairs with an average precision of ∼ 90\% and an average recall of above ∼ 76\% across a diverse array of datasets exhibiting different types of errors. This yields an average F1 improvement of more than 2\texttimes{} against state-of-the-art methods.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1190–1201},
numpages = {12}
}

@inproceedings{10.1145/2723372.2749431,
author = {Chu, Xu and Morcos, John and Ilyas, Ihab F. and Ouzzani, Mourad and Papotti, Paolo and Tang, Nan and Ye, Yin},
title = {KATARA: A Data Cleaning System Powered by Knowledge Bases and Crowdsourcing},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2749431},
doi = {10.1145/2723372.2749431},
abstract = {Classical approaches to clean data have relied on using integrity constraints, statistics, or machine learning. These approaches are known to be limited in the cleaning accuracy, which can usually be improved by consulting master data and involving experts to resolve ambiguity. The advent of knowledge bases KBs both general-purpose and within enterprises, and crowdsourcing marketplaces are providing yet more opportunities to achieve higher accuracy at a larger scale. We propose KATARA, a knowledge base and crowd powered data cleaning system that, given a table, a KB, and a crowd, interprets table semantics to align it with the KB, identifies correct and incorrect data, and generates top-k possible repairs for incorrect data. Experiments show that KATARA can be applied to various datasets and KBs, and can efficiently annotate data and suggest possible repairs.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {1247–1261},
numpages = {15},
keywords = {knowledge base, data quality, data cleaning, crowdsourcing},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@inproceedings{10.1145/2463676.2465327,
author = {Dallachiesa, Michele and Ebaid, Amr and Eldawy, Ahmed and Elmagarmid, Ahmed and Ilyas, Ihab F. and Ouzzani, Mourad and Tang, Nan},
title = {NADEEF: a commodity data cleaning system},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2465327},
doi = {10.1145/2463676.2465327},
abstract = {Despite the increasing importance of data quality and the rich theoretical and practical contributions in all aspects of data cleaning, there is no single end-to-end off-the-shelf solution to (semi-)automate the detection and the repairing of violations w.r.t. a set of heterogeneous and ad-hoc quality constraints. In short, there is no commodity platform similar to general purpose DBMSs that can be easily customized and deployed to solve application-specific data quality problems. In this paper, we present NADEEF, an extensible, generalized and easy-to-deploy data cleaning platform. NADEEF distinguishes between a programming interface and a core to achieve generality and extensibility. The programming interface allows the users to specify multiple types of data quality rules, which uniformly define what is wrong with the data and (possibly) how to repair it through writing code that implements predefined classes. We show that the programming interface can be used to express many types of data quality rules beyond the well known CFDs (FDs), MDs and ETL rules. Treating user implemented interfaces as black-boxes, the core provides algorithms to detect errors and to clean data. The core is designed in a way to allow cleaning algorithms to cope with multiple rules holistically, i.e. detecting and repairing data errors without differentiating between various types of rules. We showcase two implementations for core repairing algorithms. These two implementations demonstrate the extensibility of our core, which can also be replaced by other user-provided algorithms. Using real-life data, we experimentally verify the generality, extensibility, and effectiveness of our system.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {541–552},
numpages = {12},
keywords = {conditional functional dependency, data cleaning, etl, matching dependency},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@article{Krishnan2017,
  author = {Sanjay Krishnan and Michael J. Franklin and Ken Goldberg and Eugene Wu},
  title = {BoostClean: Automated Error Detection and Repair for Machine Learning},
  journal = {arXiv preprint arXiv:1711.01299},
  year = {2017},
  archivePrefix = {arXiv},
  primaryClass = {cs.DB},
  url = {https://doi.org/10.48550/arXiv.1711.01299}
}

@inproceedings{10.1145/3357384.3358129,
author = {Neutatz, Felix and Mahdavi, Mohammad and Abedjan, Ziawasch},
title = {ED2: A Case for Active Learning in Error Detection},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358129},
doi = {10.1145/3357384.3358129},
abstract = {State-of-the-art approaches formulate error detection as a semi-supervised classification problem. Recent research suggests that active learning is insufficiently effective for error detection and proposes the usage of neural networks and data augmentation to reduce the number of these user-provided labels. However, we can show that using the appropriate active learning strategy, it is possible to outperform the more complex models that rely on data augmentation. To this end, we propose a multi-classifier approach with two-stage sampling for active learning. This intuitive and neat sampling method chooses the most promising cells across rows and columns for labeling. On three datasets, ED2 achieves state-of-the-art detection accuracy while for large datasets, the required number of user labels is lower by one order of magnitude compared to the state of the art.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2249–2252},
numpages = {4},
keywords = {example-driven error detection, error detection, data quality, active learning},
location = {Beijing, China},
series = {CIKM '19}
}

@INPROCEEDINGS{10346079,
  author={Panchal, Deven and Baran, Isilay and Musgrove, Dan and Lu, David},
  booktitle={2023 IEEE International Conference on Internet of Things and Intelligence Systems (IoTaIS)}, 
  title={MLOps: Automatic, Zero-Touch and Reusable Machine Learning Training and Serving Pipelines}, 
  year={2023},
  volume={},
  number={},
  pages={175-181},
  keywords={Training;Cloud computing;Forensics;Pipelines;Production;Software;Data models;Acumos;Machine Learning;Deep Learning;MLOps;Platform;ML Pipelines;Microservices;Nifi;Reusable ML;Open Source;AI4EU},
  doi={10.1109/IoTaIS60147.2023.10346079}}

@article{SINGH2024102799,
title = {A comprehensive review of clustering techniques in artificial intelligence for knowledge discovery: Taxonomy, challenges, applications and future prospects},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102799},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102799},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624004476},
author = {Jaswinder Singh and Damanpreet Singh},
keywords = {Machine learning, Unsupervised learning, Data clustering, Pattern recognition, Automatic data clustering, Hybrid clustering},
abstract = {Clustering is a set of essential mathematical techniques in artificial intelligence and machine learning for analyzing massive amounts of data generated by applications. Clustering uses data mining approaches to group data points based on their intrinsic characteristics or similarity measures. These measures are essential for data mining tasks such as information retrieval, pattern recognition, classifications, and clustering from the raw data. However, significant efforts are needed to select the appropriate similarity metrics based on the distribution of data points and problem domains. Various clustering approaches and techniques are actively utilized in a variety of disciplines of research, including data science, machine learning, computer science, pattern recognition, computer vision, etc. Some partitional clustering techniques, such as K-Means and density-based clustering, etc., take some sensitive parameters from the user prior to grouping the objects, and variations on these parameters may lead to different results for the same dataset. Some traditional statistical techniques for clustering are unable to find optimal clusters or handle high-dimensional data effectively. The need for a priori requirements of elements, for instance, the number of groups, termination at the local optimum, and expensive computations are some of its limitations that must be solved. To get beyond the aforementioned restrictions, innovative, adaptable, efficient, and hybrid clustering algorithms must be created. This study provides a comprehensive review of the literature on traditional and novel clustering techniques in a cohesive manner, their trending applications in various domains, their summarization, challenges, and future scope. In addition, data clustering embraces various scientific disciplines. Thus, this study will be beneficial and will provide an effective reference point for progressive researchers, analysts, and artificial intelligence professionals to develop novel, flexible, and efficient state-of-the-art clustering techniques.}
}

@article{IKOTUN2023178,
title = {K-means clustering algorithms: A comprehensive review, variants analysis, and advances in the era of big data},
journal = {Information Sciences},
volume = {622},
pages = {178-210},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.11.139},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522014633},
author = {Abiodun M. Ikotun and Absalom E. Ezugwu and Laith Abualigah and Belal Abuhaija and Jia Heming},
keywords = {K-means, K-means variants, Clustering algorithm, Modified k-means, Improved k-means, Perspectives on big data clustering, Big data clustering},
abstract = {Advances in recent techniques for scientific data collection in the era of big data allow for the systematic accumulation of large quantities of data at various data-capturing sites. Similarly, exponential growth in the development of different data analysis approaches has been reported in the literature, amongst which the K-means algorithm remains the most popular and straightforward clustering algorithm. The broad applicability of the algorithm in many clustering application areas can be attributed to its implementation simplicity and low computational complexity. However, the K-means algorithm has many challenges that negatively affect its clustering performance. In the algorithm’s initialization process, users must specify the number of clusters in a given dataset apriori while the initial cluster centers are randomly selected. Furthermore, the algorithm's performance is susceptible to the selection of this initial cluster and for large datasets, determining the optimal number of clusters to start with becomes complex and is a very challenging task. Moreover, the random selection of the initial cluster centers sometimes results in minimal local convergence due to its greedy nature. A further limitation is that certain data object features are used in determining their similarity by using the Euclidean distance metric as a similarity measure, but this limits the algorithm’s robustness in detecting other cluster shapes and poses a great challenge in detecting overlapping clusters. Many research efforts have been conducted and reported in literature with regard to improving the K-means algorithm’s performance and robustness. The current work presents an overview and taxonomy of the K-means clustering algorithm and its variants. The history of the K-means, current trends, open issues and challenges, and recommended future research perspectives are also discussed.}
}

@article{Ran2023,
  author = {Ran, X. and Xi, Y. and Lu, Y. and others},
  title = {Comprehensive survey on hierarchical clustering algorithms and the recent developments},
  journal = {Artificial Intelligence Review},
  volume = {56},
  pages = {8219--8264},
  year = {2023},
  month = {August},
  doi = {10.1007/s10462-022-10366-3},
  url = {https://doi.org/10.1007/s10462-022-10366-3}
}

@article{10.1145/3299876,
author = {Barton, Tomas and Bruna, Tomas and Kordik, Pavel},
title = {Chameleon 2: An Improved Graph-Based Clustering Algorithm},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3299876},
doi = {10.1145/3299876},
abstract = {Traditional clustering algorithms fail to produce human-like results when confronted with data of variable density, complex distributions, or in the presence of noise. We propose an improved graph-based clustering algorithm called Chameleon 2, which overcomes several drawbacks of state-of-the-art clustering approaches. We modified the internal cluster quality measure and added an extra step to ensure algorithm robustness. Our results reveal a significant positive impact on the clustering quality measured by Normalized Mutual Information on 32 artificial datasets used in the clustering literature. This significant improvement is also confirmed on real-world datasets.The performance of clustering algorithms such as DBSCAN is extremely parameter sensitive, and exhaustive manual parameter tuning is necessary to obtain a meaningful result. All hierarchical clustering methods are very sensitive to cutoff selection, and a human expert is often required to find the true cutoff for each clustering result. We present an automated cutoff selection method that enables the Chameleon 2 algorithm to generate high-quality clustering in autonomous mode.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {10},
numpages = {27},
keywords = {Cluster analysis, clustering, graph clustering, pattern recognition}
}

@article{Cai2016,
  author = {Fan Cai and Nhien-An Le-Khac and Tahar Kechadi},
  title = {Clustering Approaches for Financial Data Analysis: a Survey},
  journal = {arXiv preprint arXiv:1609.08520},
  year = {2016},
  archivePrefix = {arXiv},
  primaryClass = {q-fin.GN},
  url = {https://doi.org/10.48550/arXiv.1609.08520}
}

@article{Blumenberg2020,
  author = {Lili Blumenberg and Kelly V. Ruggles},
  title = {Hypercluster: a flexible tool for parallelized unsupervised clustering optimization},
  journal = {BMC Bioinformatics},
  volume = {21},
  number = {1},
  pages = {428},
  year = {2020},
  month = {September},
  doi = {10.1186/s12859-020-03774-1},
  url = {https://doi.org/10.1186/s12859-020-03774-1},
  issn = {1471-2105}
}

@article{BOLANOSMARTINEZ2024102164,
title = {Clustering pipeline for vehicle behavior in smart villages},
journal = {Information Fusion},
volume = {104},
pages = {102164},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.102164},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523004803},
author = {Daniel Bolaños-Martinez and Maria Bermudez-Edo and Jose Luis Garrido},
keywords = {Internet of Things (IoT), Sensors, Clustering, Smart villages, Explainability},
abstract = {Smart cities and villages present a plethora of opportunities for fusing and managing multi-source data. However, in the analysis of mobility patterns, the use of only one data source (i.e., road sensors) without considering other contextual data sources, limits the understanding of the process. To address this gap, we propose a pipeline that integrates multiple data sources, providing valuable information for pattern extraction, mainly based on vehicle mobility behavior and provenance. Our research also highlights the critical role of selecting the appropriate normalization algorithm to scale input features from heterogeneous data sources, which has not received sufficient attention in the literature. We conducted our analysis using data from four License Plate Recognition (LPR) cameras, spanning nine months, and incorporating several databases that include provenance, gross income, and holiday information, resulting in a dataset of over 50,000 vehicles. Using this data and our clustering pipeline, we identified various traffic patterns among residents and visitors in a rural touristic area. Our findings assist data analysts in choosing algorithms for analyzing heterogeneous datasets. Moreover, policymakers could use our results to adjust the resources, such as new parking zones.}
}

@article{SALEHIN202452,
title = {AutoML: A systematic review on automated machine learning with neural architecture search},
journal = {Journal of Information and Intelligence},
volume = {2},
number = {1},
pages = {52-81},
year = {2024},
issn = {2949-7159},
doi = {https://doi.org/10.1016/j.jiixd.2023.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S2949715923000604},
author = {Imrus Salehin and Md. Shamiul Islam and Pritom Saha and S.M. Noman and Azra Tuni and Md. Mehedi Hasan and Md. Abu Baten},
keywords = {AutoML, Neural architecture search, Advance machine learning, Search space, Hyperparameter optimization},
abstract = {AutoML (Automated Machine Learning) is an emerging field that aims to automate the process of building machine learning models. AutoML emerged to increase productivity and efficiency by automating as much as possible the inefficient work that occurs while repeating this process whenever machine learning is applied. In particular, research has been conducted for a long time on technologies that can effectively develop high-quality models by minimizing the intervention of model developers in the process from data preprocessing to algorithm selection and tuning. In this semantic review research, we summarize the data processing requirements for AutoML approaches and provide a detailed explanation. We place greater emphasis on neural architecture search (NAS) as it currently represents a highly popular sub-topic within the field of AutoML. NAS methods use machine learning algorithms to search through a large space of possible architectures and find the one that performs best on a given task. We provide a summary of the performance achieved by representative NAS algorithms on the CIFAR-10, CIFAR-100, ImageNet and well-known benchmark datasets. Additionally, we delve into several noteworthy research directions in NAS methods including one/two-stage NAS, one-shot NAS and joint hyperparameter with architecture optimization. We discussed how the search space size and complexity in NAS can vary depending on the specific problem being addressed. To conclude, we examine several open problems (SOTA problems) within current AutoML methods that assure further investigation in future research.}
}

@INPROCEEDINGS{6544847,
  author={Chu, Xu and Ilyas, Ihab F. and Papotti, Paolo},
  booktitle={2013 IEEE 29th International Conference on Data Engineering (ICDE)}, 
  title={Holistic data cleaning: Putting violations into context}, 
  year={2013},
  volume={},
  number={},
  pages={458-469},
  keywords={Maintenance engineering;Databases;Cleaning;Context;Cities and towns;Remuneration;Proposals},
  doi={10.1109/ICDE.2013.6544847}}

@article{10.14778/3407790.3407801,
author = {Mahdavi, Mohammad and Abedjan, Ziawasch},
title = {Baran: effective error correction via a unified context representation and transfer learning},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3407790.3407801},
doi = {10.14778/3407790.3407801},
abstract = {Traditional error correction solutions leverage handmaid rules or master data to find the correct values. Both are often amiss in real-world scenarios. Therefore, it is desirable to additionally learn corrections from a limited number of example repairs. To effectively generalize example repairs, it is necessary to capture the entire context of each erroneous value. A context comprises the value itself, the co-occurring values inside the same tuple, and all values that define the attribute type. Typically, an error corrector based on any of these context information undergoes an individual process of operations that is not always easy to integrate with other types of error correctors. In this paper, we present a new error correction system, Baran, which provides a unifying abstraction for integrating multiple error corrector models that can be pretrained and updated in the same way. Because of the holistic nature of our approach, we generate more correction candidates than state of the art and, because of the underlying context-aware data representation, we achieve high precision. We show that, by pretraining our models based on Wikipedia revisions, our system can further improve its overall precision and recall. In our experiments, Baran significantly outperforms state-of-the-art error correction systems in terms of effectiveness and human involvement requiring only 20 labeled tuples.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {1948–1961},
numpages = {14}
}

@INPROCEEDINGS{6544854,
  author={Beskales, George and Ilyas, Ihab F. and Golab, Lukasz and Galiullin, Artur},
  booktitle={2013 IEEE 29th International Conference on Data Engineering (ICDE)}, 
  title={On the relative trust between inconsistent data and inaccurate constraints}, 
  year={2013},
  volume={},
  number={},
  pages={541-552},
  keywords={Maintenance engineering;Optimized production technology;Semantics;Approximation algorithms;Cleaning;Measurement;Vectors},
  doi={10.1109/ICDE.2013.6544854}}

@INPROCEEDINGS{5767833,
  author={Chiang, Fei and Miller, Renée J.},
  booktitle={2011 IEEE 27th International Conference on Data Engineering}, 
  title={A unified model for data and constraint repair}, 
  year={2011},
  volume={},
  number={},
  pages={446-457},
  keywords={Maintenance engineering;Data models;Computational modeling;Redundancy;Databases;Semantics;Cities and towns},
  doi={10.1109/ICDE.2011.5767833}}

@article{10.1093/bioinformatics/btr597,
    author = {Stekhoven, Daniel J. and Bühlmann, Peter},
    title = {MissForest—non-parametric missing value imputation for mixed-type data},
    journal = {Bioinformatics},
    volume = {28},
    number = {1},
    pages = {112-118},
    year = {2011},
    month = {10},
    abstract = {Motivation: Modern data acquisition based on high-throughput technology is often facing the problem of missing data. Algorithms commonly used in the analysis of such large-scale data often depend on a complete set. Missing value imputation offers a solution to this problem. However, the majority of available imputation methods are restricted to one type of variable only: continuous or categorical. For mixed-type data, the different types are usually handled separately. Therefore, these methods ignore possible relations between variable types. We propose a non-parametric method which can cope with different types of variables simultaneously.Results: We compare several state of the art methods for the imputation of missing values. We propose and evaluate an iterative imputation method (missForest) based on a random forest. By averaging over many unpruned classification or regression trees, random forest intrinsically constitutes a multiple imputation scheme. Using the built-in out-of-bag error estimates of random forest, we are able to estimate the imputation error without the need of a test set. Evaluation is performed on multiple datasets coming from a diverse selection of biological fields with artificially introduced missing values ranging from 10\% to 30\%. We show that missForest can successfully handle missing values, particularly in datasets including different types of variables. In our comparative study, missForest outperforms other methods of imputation especially in data settings where complex interactions and non-linear relations are suspected. The out-of-bag imputation error estimates of missForest prove to be adequate in all settings. Additionally, missForest exhibits attractive computational efficiency and can cope with high-dimensional data.Availability: The ℝ package missForest is freely available from http://stat.ethz.ch/CRAN/.Contact:  stekhoven@stat.math.ethz.ch; buhlmann@stat.math.ethz.ch},
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btr597},
    url = {https://doi.org/10.1093/bioinformatics/btr597},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/28/1/112/50568519/bioinformatics\_28\_1\_112.pdf},
}

@article{Hu2017,
  author = {W. Hu and A. Zaveri and H. Qiu and others},
  title = {Cleaning by clustering: methodology for addressing data quality issues in biomedical metadata},
  journal = {BMC Bioinformatics},
  volume = {18},
  pages = {415},
  year = {2017},
  month = {September},
  doi = {10.1186/s12859-017-1832-4},
  url = {https://doi.org/10.1186/s12859-017-1832-4},
  publisher = {Springer Nature}
}

@article{Bernhardt2022,
  author = {M. Bernhardt and D. C. Castro and R. Tanno and others},
  title = {Active label cleaning for improved dataset quality under resource constraints},
  journal = {Nature Communications},
  volume = {13},
  pages = {1161},
  year = {2022},
  month = {March},
  doi = {10.1038/s41467-022-28818-3},
  url = {https://doi.org/10.1038/s41467-022-28818-3},
  publisher = {Springer Nature}
}

@INPROCEEDINGS{10664059,
  author={Puri, Digvijay and Gupta, Dr. Deepak},
  booktitle={2024 International Conference on Emerging Innovations and Advanced Computing (INNOCOMP)}, 
  title={Enhancing K-Means Clustering with Data-Driven Initialization and Adaptive Distance Measures}, 
  year={2024},
  volume={},
  number={},
  pages={570-574},
  keywords={Measurement;Technological innovation;Accuracy;Heuristic algorithms;Clustering algorithms;Stability analysis;Partitioning algorithms;K-mean;clustering method;comparative analysis},
  doi={10.1109/INNOCOMP63224.2024.00099}}

@ARTICLE{8896034,
  author={Lai, Yongxuan and He, Songyao and Lin, Zhijie and Yang, Fan and Zhou, Qifeng and Zhou, Xiaofang},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={An Adaptive Robust Semi-Supervised Clustering Framework Using Weighted Consensus of Random $k$k-Means Ensemble}, 
  year={2021},
  volume={33},
  number={5},
  pages={1877-1890},
  keywords={Clustering algorithms;Partitioning algorithms;Libraries;Noise measurement;Linear programming;Fans;Heuristic algorithms;Cluster ensemble;semi-supervised clustering;pairwise constraint;k-means},
  doi={10.1109/TKDE.2019.2952596}}

@ARTICLE{9151362,
  author={Ge, Congcong and Gao, Yunjun and Miao, Xiaoye and Yao, Bin and Wang, Haobo},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Hybrid Data Cleaning Framework Using Markov Logic Networks}, 
  year={2022},
  volume={34},
  number={5},
  pages={2048-2062},
  keywords={Cleaning;Maintenance engineering;Dictionaries;Indexes;Markov processes;Integrated circuits;Hospitals;Data cleaning;integrity constraints;markov logic network},
  doi={10.1109/TKDE.2020.3012472}}

@article{10.14778/2994509.2994514,
author = {Krishnan, Sanjay and Wang, Jiannan and Wu, Eugene and Franklin, Michael J. and Goldberg, Ken},
title = {ActiveClean: interactive data cleaning for statistical modeling},
year = {2016},
issue_date = {August 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/2994509.2994514},
doi = {10.14778/2994509.2994514},
abstract = {Analysts often clean dirty data iteratively--cleaning some data, executing the analysis, and then cleaning more data based on the results. We explore the iterative cleaning process in the context of statistical model training, which is an increasingly popular form of data analytics. We propose ActiveClean, which allows for progressive and iterative cleaning in statistical modeling problems while preserving convergence guarantees. ActiveClean supports an important class of models called convex loss models (e.g., linear regression and SVMs), and prioritizes cleaning those records likely to affect the results. We evaluate ActiveClean on five real-world datasets UCI Adult, UCI EEG, MNIST, IMDB, and Dollars For Docs with both real and synthetic errors. The results show that our proposed optimizations can improve model accuracy by up-to 2.5x for the same amount of data cleaned. Furthermore for a fixed cleaning budget and on all real dirty datasets, ActiveClean returns more accurate models than uniform sampling and Active Learning.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {948–959},
numpages = {12}
}

@article{YANG2020507,
title = {Research on Clustering Method Based on Weighted Distance Density and K-Means},
journal = {Procedia Computer Science},
volume = {166},
pages = {507-511},
year = {2020},
note = {Proceedings of the 3rd International Conference on Mechatronics and Intelligent Robotics (ICMIR-2019)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.056},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920301782},
author = {Wei Yang and Hua Long and Lihua Ma and Huifang Sun},
keywords = {K-means algorithm, density calculation, weighted distance, cluster centroid},
abstract = {In this paper, the effect of the initial clustering center selection on the performance of the K-means algorithm is studied, and the performance of the algorithm is enhanced through better initialization techniques. In the K-means clustering process, when calculating the density of a data set by using a weighted distance density calculation method, significant improvement in the defects of poor clustering results caused by the local optimum and large intra-cluster variance in the traditional K-means clustering algorithm has been found. Experimental results show that by using the improved method proposed in this paper, the intra-cluster variance of clustering results is reduced by 15.5% compared with the traditional method, which makes great improvement in the performance of the algorithm.}
}

@article{HUANG2021107996,
title = {Robust deep k-means: An effective and simple method for data clustering},
journal = {Pattern Recognition},
volume = {117},
pages = {107996},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.107996},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321001837},
author = {Shudong Huang and Zhao Kang and Zenglin Xu and Quanhui Liu},
keywords = {K-Means algorithm, Robust clustering, Deep learning},
abstract = {Clustering aims to partition an input dataset into distinct groups according to some distance or similarity measurements. One of the most widely used clustering method nowadays is the k-means algorithm because of its simplicity and efficiency. In the last few decades, k-means and its various extensions have been formulated to solve the practical clustering problems. However, existing clustering methods are often presented in a single-layer formulation (i.e., shallow formulation). As a result, the mapping between the obtained low-level representation and the original input data may contain rather complex hierarchical information. To overcome the drawbacks of low-level features, deep learning techniques are adopted to extract deep representations and improve the clustering performance. In this paper, we propose a robust deep k-means model to learn the hidden representations associate with different implicit lower-level attributes. By using the deep structure to hierarchically perform k-means, the hierarchical semantics of data can be exploited in a layerwise way. Data samples from the same class are forced to be closer layer by layer, which is beneficial for clustering task. The objective function of our model is derived to a more trackable form such that the optimization problem can be tackled more easily and the final robust results can be obtained. Experimental results over 12 benchmark data sets substantiate that the proposed model achieves a breakthrough in clustering performance, compared with both classical and state-of-the-art methods.}
}

@INPROCEEDINGS{10486339,
  author={Kulkarni, Omkaresh and Burhanpurwala, Adnan},
  booktitle={2024 3rd International conference on Power Electronics and IoT Applications in Renewable Energy and its Control (PARC)}, 
  title={A Survey of Advancements in DBSCAN Clustering Algorithms for Big Data}, 
  year={2024},
  volume={},
  number={},
  pages={106-111},
  keywords={Surveys;Renewable energy sources;Machine learning algorithms;Shape;Reviews;Noise;Clustering algorithms;clustering algorithm;Density based clustering;DBSCAN;big data;data mining},
  doi={10.1109/PARC59193.2024.10486339}}

@article{QIAN2024127329,
title = {MDBSCAN: A multi-density DBSCAN based on relative density},
journal = {Neurocomputing},
volume = {576},
pages = {127329},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127329},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224001000},
author = {Jiaxin Qian and You Zhou and Xuming Han and Yizhang Wang},
keywords = {Relative density, DBSCAN, Multiple density clustering},
abstract = {DBSCAN is a widely used clustering algorithm based on density metrics that can efficiently identify clusters with uniform density. However, if the densities of different clusters are varying, the corresponding clustering results may be not good. To address this issue, we propose a multi-density DBSCAN based on the relative density (MDBSCAN), which can achieve better results on clusters with multiple densities. The intuition of our work is simple but effective, we first divide the dataset into two parts: low density and high density, and then we take a divide and conquer method to deal with the respective parts to avoid them interfering with each other. Specifically, the proposed MDBSCAN consists of three steps: (i) extract the low-density data points in the dataset by relative density. (ii) find natural clusters among the identified low-density data points. (iii) clustering the remaining data points (except the data points of natural clusters in a dataset) by using DBSCAN and assigning the noises (generated by DBSCAN) to the nearest clusters. To verify the effectiveness of the proposed MDBSCAN algorithm, we conduct experiments on ten synthetic datasets and six real-world datasets. Experimental results demonstrate that the proposed MDBSCAN algorithm outperforms the original DBSCAN and six extends of DBSCAN, especially including two state-of-the-art algorithms (DRL-DBSCAN and AMD-DBSCAN) in most cases.}
}

@article{CHENG2024120731,
title = {GB-DBSCAN: A fast granular-ball based DBSCAN clustering algorithm},
journal = {Information Sciences},
volume = {674},
pages = {120731},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120731},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524006443},
author = {Dongdong Cheng and Cheng Zhang and Ya Li and Shuyin Xia and Guoyin Wang and Jinlong Huang and Sulan Zhang and Jiang Xie},
keywords = {DBSCAN, Granular-ball, KNN, Clustering},
abstract = {Density-Based Spatial Clustering of Applications with Noise (DBSCAN) identifies high-density connected areas as clusters, so that it has advantages in discovering arbitrary-shaped clusters. However, it has difficulty in adjusting parameters and since it needs to scan all data points in turn, its time complexity is O(n2). Granular-ball (GB) is a coarse grained representation of data. It is on the basis of the assumption that an object and its local neighbors have similar distribution and they have high possibility of belonging to the same class. It has been introduced into supervised learning by Xia et al. to improve the efficiency of supervised learning. Inspired by the idea of granular-ball, we introduce it into unsupervised learning and use it to improve the efficiency of DBSCAN, called GB-DBSCAN. The main idea of the proposed algorithm GB-DBSCAN is to employ granular-ball to represent a set of data points and then clustering on granular-balls, instead of the data points. Firstly, we use k-nearest neighbors (KNN) to generate granular-balls, which is a bottom-up strategy, and describe granular-balls according to their centers and radius. Then, the granular-balls are divided into Core-GBs and Non-Core-GBs according to their density. After that, the Core-GBs are merged into clusters according to the idea of DBSCAN and the Non-Core-GBs are assigned to the appropriate clusters. Since the granular-balls' number is much smaller than the size of the objects in a dataset, the running time of DBSCAN is greatly reduced. By comparing with KNN-BLOCK DBSCAN, RNN-DBSCAN, DBSCAN, K-means, DP and SNN-DPC algorithms, the proposed algorithm can get similar or even better clustering result in much less running time.}
}

@article{HAJIHOSSEINLOU2024126094,
title = {A comprehensive evaluation of OPTICS, GMM and K-means clustering methodologies for geochemical anomaly detection connected with sample catchment basins},
journal = {Geochemistry},
volume = {84},
number = {2},
pages = {126094},
year = {2024},
issn = {0009-2819},
doi = {https://doi.org/10.1016/j.chemer.2024.126094},
url = {https://www.sciencedirect.com/science/article/pii/S0009281924000187},
author = {Mahsa Hajihosseinlou and Abbas Maghsoudi and Reza Ghezelbash},
keywords = {OPTICS algorithm, Gaussian Mixture Model, Anomaly detection, Geochemical clustering, Machine learning},
abstract = {The process of data-driven clustering to uncover geochemical anomalies linked to sample catchment basins (SCBs) includes a comprehensive framework to discern areas exhibiting unique geochemical attributes within a specified study area. The Ordering Points to Identify the Clustering Structure (OPTICS) method can serve as a robust methodology for detecting geochemical anomalies in SCBs. This is attributed to its capacity to effectively manage varying cluster densities, adaptively identify cluster numbers, exhibit resilience to noise, and display minimum sensitivity to parameters. A comparison was conducted in this research between the outcomes of the OPTICS clustering algorithm and two traditional clustering techniques, namely the Gaussian Mixture Model (GMM) and K-means clustering. In the following, the Expectation-Maximization (EM) technique is employed to train the GMM for clustering. Moreover, the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) as two validate statistical metrics implemented to ascertain the optimal number of components (clusters) belong to the GMM. It should be noted that the effectiveness of the clustering algorithms was further assessed using the Calinski-Harabasz (CH) index and the success-rate curves. OPTICS, a density-based clustering approach, was confirmed to be more effective than K-means and GMM for identifying MVT PbZn anomalies in Varcheh district, western Iran. Furthermore, the specified anomalies show a geo-spatial correspondence with the geological facts, and it has been observed that strong anomalies are more discoverable in close proximity to MVT PbZn occurrences. This work suggests a novel anomaly detection approach based on OPTICS, which exhibits superior performance and data-modeling efficiency. The main emphasis is on effectively distinguishing geochemical anomalies from sample data originating from populations with uncertain distributions.}
}

@article{10.3233/IDA-205497,
author = {Tang, Chunhua and Wang, Han and Wang, Zhiwen and Zeng, Xiangkun and Yan, Huaran and Xiao, Yingjie},
title = {An improved OPTICS clustering algorithm for discovering clusters with uneven densities},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {25},
number = {6},
issn = {1088-467X},
url = {https://doi.org/10.3233/IDA-205497},
doi = {10.3233/IDA-205497},
abstract = {Most density-based clustering algorithms have the problems of difficult parameter setting, high time complexity, poor noise recognition, and weak clustering for datasets with uneven density. To solve these problems, this paper proposes FOP-OPTICS algorithm (Finding of the Ordering Peaks Based on OPTICS), which is a substantial improvement of OPTICS (Ordering Points To Identify the Clustering Structure). The proposed algorithm finds the demarcation point (DP) from the Augmented Cluster-Ordering generated by OPTICS and uses the reachability-distance of DP as the radius of neighborhood eps of its corresponding cluster. It overcomes the weakness of most algorithms in clustering datasets with uneven densities. By computing the distance of the k-nearest neighbor of each point, it reduces the time complexity of OPTICS; by calculating density-mutation points within the clusters, it can efficiently recognize noise. The experimental results show that FOP-OPTICS has the lowest time complexity, and outperforms other algorithms in parameter setting and noise recognition.},
journal = {Intell. Data Anal.},
month = jan,
pages = {1453–1471},
numpages = {19},
keywords = {OPTICS, FOP-OPTICS, clustering algorithm, noise identification}
}

@article{KAMIL20232625,
title = {Enhancement of OPTICS’ time complexity by using fuzzy clusters},
journal = {Materials Today: Proceedings},
volume = {80},
pages = {2625-2630},
year = {2023},
note = {SI:5 NANO 2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.06.441},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321048355},
author = {Israa S. Kamil and Safaa O. Al-Mamory},
keywords = {Data mining, Density-based clustering, OPTICS, Time complexity, Fuzzy clustering},
abstract = {Density-Based clustering are the main clustering algorithms because they can cluster data with different shapes and densities, but some of these algorithms have high time complexity like OPTICS (Ordering Points to Identify Clustering Structure) and DBSCAN (Density-based spatial clustering of applications with noise) where the time complexity up to O(n2). In this paper, we use an approach to reduce this complexity by providing fuzzy clusters to OPTICS, which make the process of finding neighbours within an only narrow region (fuzzy group) instead of searching all the state space making the algorithm faster than the original one with keeping almost the same accuracy of the innovative algorithm. The results show that there is an improvement in execution time using some synthetic and real datasets with high dimensions.}
}

@article{CHEN2025125714,
title = {Hierarchical clustering algorithm based on Crystallized neighborhood graph for identifying complex structured datasets},
journal = {Expert Systems with Applications},
volume = {265},
pages = {125714},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125714},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424025818},
author = {Zhongshang Chen and Ji Feng and Degang Yang and Fapeng Cai},
keywords = {Clustering, Data mining, Crystallized neighborhood graph, Weight shared natural neighborhood graph, Merging strategy of sub-graphs},
abstract = {In data mining, the neighborhood graph is an important method for describing the distribution of datasets. However, existing neighborhood graph methods are often sensitive to parameters settings and the presence of outliers. These traditional neighborhood graphs typically necessitate one or more input parameters, and they may not function optimally when applied to complex datasets that include a substantial number of noise points. To overcome these drawbacks, we have received inspiration from crude salt purification, and propose a non-parameter neighborhood graph method named the Crystallized neighborhood graph (CNG). This method can adaptively capture the distribution structure of complex structured datasets. Based on the CNG, we propose a Hierarchical clustering algorithm based on the Crystallized neighborhood graph for identifying complex structured datasets (HCCNG). It redefines the similarity between sub-graphs using the bridges between sub-graphs and the shortest distance between them. Then, sub-graphs are repeatedly merged according to their similarity until the ideal clustering result is achieved. The experimental results show that the HCCNG algorithm can identify not only popular clusters, but also variable-density spherical clusters. Moreover, it performs well on complex structured datasets with a significant amount of noise.}
}

@article{Abdelaal2023,
  author = {Mohamed Abdelaal and Christian Hammacher and Harald Schoening},
  title = {REIN: A Comprehensive Benchmark Framework for Data Cleaning Methods in ML Pipelines},
  journal = {arXiv preprint arXiv:2302.04702},
  year = {2023},
  archivePrefix = {arXiv},
  primaryClass = {cs.DB},
  url = {https://doi.org/10.48550/arXiv.2302.04702}
}

@article{Barbudo2023,
  author = {Rafael Barbudo and Sebastián Ventura and José Raúl Romero},
  title = {Eight years of AutoML: categorisation, review and trends},
  journal = {Knowledge and Information Systems},
  volume = {65},
  number = {12},
  pages = {5097--5149},
  year = {2023},
  month = {December},
  doi = {10.1007/s10115-023-01935-1},
  url = {https://doi.org/10.1007/s10115-023-01935-1},
  issn = {0219-3116}
}

@article{ELDEEB2024122877,
title = {AutoMLBench: A comprehensive experimental evaluation of automated machine learning frameworks},
journal = {Expert Systems with Applications},
volume = {243},
pages = {122877},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122877},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423033791},
author = {Hassan Eldeeb and Mohamed Maher and Radwa Elshawi and Sherif Sakr},
keywords = {Automated machine learning, Meta-learning, Search space, Ensemble, Time budget},
abstract = {With the booming demand for machine learning applications, it has been recognized that the number of knowledgeable data scientists cannot scale with the growing data volumes and application needs in our digital world. In response to this demand, several automated machine learning (AutoML) frameworks have been developed to fill the gap of human expertise by automating the process of building machine learning pipelines. Each framework comes with different heuristics-based design decisions. In this study, we present a comprehensive evaluation and comparison of the performance characteristics of six popular AutoML frameworks, namely, AutoWeka, AutoSKlearn, TPOT, Recipe, ATM and SmartML across 100 data sets from established AutoML benchmark suites. Our experimental evaluation considers different aspects for its comparison, including the performance impact of several design decisions, including time budget, size of search space, meta-learning, and ensemble construction. The results of our study reveal various interesting insights that can significantly guide and impact the design of AutoML frameworks.}
}

@article{Conrad2022,
  author = {Felix Conrad and Mauritz Mälzer and Michael Schwarzenberger and Hajo Wiemer and Steffen Ihlenfeldt},
  title = {Benchmarking AutoML for regression tasks on small tabular data in materials design},
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {19350},
  year = {2022},
  month = {November},
  doi = {10.1038/s41598-022-23327-1},
  url = {https://doi.org/10.1038/s41598-022-23327-1},
  issn = {2045-2322}
}

@article{Alsharef2022,
  author = {Ahmad Alsharef and Karan Aggarwal and Sonia and Manoj Kumar and Ashutosh Mishra},
  title = {Review of ML and AutoML Solutions to Forecast Time-Series Data},
  journal = {Archives of Computational Methods in Engineering},
  volume = {29},
  number = {7},
  pages = {5297--5311},
  year = {2022},
  month = {June},
  doi = {10.1007/s11831-022-09765-0},
  url = {https://doi.org/10.1007/s11831-022-09765-0},
  pmid = {35669518},
  pmc = {PMC9159649},
  publisher = {Springer}
}

@article{HE2021106622,
title = {AutoML: A survey of the state-of-the-art},
journal = {Knowledge-Based Systems},
volume = {212},
pages = {106622},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.106622},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120307516},
author = {Xin He and Kaiyong Zhao and Xiaowen Chu},
keywords = {Deep learning, Automated machine learning (autoML), Neural architecture search (NAS), Hyperparameter optimization (HPO)},
abstract = {Deep learning (DL) techniques have obtained remarkable achievements on various tasks, such as image recognition, object detection, and language modeling. However, building a high-quality DL system for a specific task highly relies on human expertise, hindering its wide application. Meanwhile, automated machine learning (AutoML) is a promising solution for building a DL system without human assistance and is being extensively studied. This paper presents a comprehensive and up-to-date review of the state-of-the-art (SOTA) in AutoML. According to the DL pipeline, we introduce AutoML methods – covering data preparation, feature engineering, hyperparameter optimization, and neural architecture search (NAS) – with a particular focus on NAS, as it is currently a hot sub-topic of AutoML. We summarize the representative NAS algorithms’ performance on the CIFAR-10 and ImageNet datasets and further discuss the following subjects of NAS methods: one/two-stage NAS, one-shot NAS, joint hyperparameter and architecture optimization, and resource-aware NAS. Finally, we discuss some open problems related to the existing AutoML methods for future research.}
}

@article{10.1145/3643564,
author = {Poulakis, Yannis and Doulkeridis, Christos and Kyriazis, Dimosthenis},
title = {A Survey on AutoML Methods and Systems for Clustering},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3643564},
doi = {10.1145/3643564},
abstract = {Automated Machine Learning (AutoML) aims to identify the best-performing machine learning algorithm along with its input parameters for a given dataset and a specific machine learning task. This is a challenging problem, as the process of finding the best model and tuning it for a particular problem at hand is both time-consuming for a data scientist and computationally expensive. In this survey, we focus on unsupervised learning, and we turn our attention on AutoML methods for clustering. We present a systematic review that includes many recent research works for automated clustering. Furthermore, we provide a taxonomy for the classification of existing works, and we perform a qualitative comparison. As a result, this survey provides a comprehensive overview of the field of AutoML for clustering. Moreover, we identify open challenges for future research in this field.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {120},
numpages = {30},
keywords = {Unsupervised learning, clustering, automated machine learning, meta-learning, algorithm selection, hyperparameter tuning}
}

@article{10.5555/3586589.3586850,
author = {Feurer, Matthias and Eggensperger, Katharina and Falkner, Stefan and Lindauer, Marius and Hutter, Frank},
title = {Auto-sklearn 2.0: hands-free AutoML via meta-learning},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {Automated Machine Learning (AutoML) supports practitioners and researchers with the tedious task of designing machine learning pipelines and has recently achieved substantial success. In this paper, we introduce new AutoML approaches motivated by our winning submission to the second ChaLearn AutoML challenge. We develop PoSH Auto-sklearn, which enables AutoML systems to work well on large datasets under rigid time limits by using a new, simple and meta-feature-free meta-learning technique and by employing a successful bandit strategy for budget allocation. However, PoSH Auto-sklearn introduces even more ways of running AutoML and might make it harder for users to set it up correctly. Therefore, we also go one step further and study the design space of AutoML itself, proposing a solution towards truly hands-free AutoML. Together, these changes give rise to the next generation of our AutoML system, Auto-sklearn 2.0 . We verify the improvements by these additions in an extensive experimental study on 39 AutoML benchmark datasets. We conclude the paper by comparing to other popular AutoML frameworks and Auto-sklearn 1.0 , reducing the relative error by up to a factor of 4:5, and yielding a performance in 10 minutes that is substantially better than what Auto-sklearn 1.0 achieves within an hour.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {261},
numpages = {61},
keywords = {automated machine learning, hyperparameter optimization, meta-learning, automated AutoML, benchmark}
}

@inproceedings{10.1145/3674029.3674058,
author = {Singh, Akansha and Prakash, Nupur and Jain, Anurag},
title = {Chronic Diseases Prediction using two different pipelines TPOT and Genetic Algorithm based models: A Comparative analysis},
year = {2024},
isbn = {9798400716379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674029.3674058},
doi = {10.1145/3674029.3674058},
abstract = {Chronic diseases are a significant burden on public health systems worldwide, and early detection is crucial for their effective management. While automated tools such as TPOT have proven useful for predicting chronic diseases, they still have limitations such as complex pipeline structures, dependence on random search methods for hyperparameter tuning, and lengthy model building times. To address these limitations, this study compared TPOT with self-designed Genetic Algorithm (GA)-based pipelines in predicting several chronic diseases, including COPD, Cancer, Heart Disease, and Diabetes. Three GA-based pipelines were created, each encapsulating distinct machine learning models. The results revealed that GA-based pipelines outperformed TPOT in performance, achieving exceptional accuracies of 99\% and 93.4\% on the cancer and diabetes datasets, respectively. The findings suggest that GA-based models, designed with domain expertise and tailored to specific datasets, provide a more reliable and effective strategy for predicting chronic diseases. This underscores the notion that GA-based pipelines offer clinicians and practitioners a dependable means of obtaining accurate results expeditiously.},
booktitle = {Proceedings of the 2024 9th International Conference on Machine Learning Technologies},
pages = {175–180},
numpages = {6},
keywords = {ExtraTree, Genetic Algorithm, GradientBoosting, MLP, Optimization, TPOT},
location = {Oslo, Norway},
series = {ICMLT '24}
}

@inproceedings{10.1145/3638529.3654061,
author = {Kenny, Angus and Ray, Tapabrata and Limmer, Steffen and Singh, Hemant Kumar and Rodemann, Tobias and Olhofer, Markus},
title = {Using Bayesian Optimization to Improve Hyperparameter Search in TPOT},
year = {2024},
isbn = {9798400704949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638529.3654061},
doi = {10.1145/3638529.3654061},
abstract = {Automated machine learning (AutoML) has emerged as a pivotal tool for applying machine learning (ML) models to real-world problems. Tree-based pipeline optimization tool (TPOT) is an AutoML framework known for effectively solving complex tasks. TPOT's search involves two fundamental objectives: finding optimal pipeline structures (i.e., combinations of ML operators) and identifying suitable hyperparameters for these structures. While its use of genetic programming enables TPOT to excel in structural search, its hyperparameter search, involving discretization and random selection from extensive potential value ranges, can be computationally inefficient. This paper presents a novel methodology that heavily restricts the initial hyperparameter search space, directing TPOT's focus towards structural exploration. As the search evolves, Bayesian optimization (BO) is used to refine the hyperparameter space based on data from previous pipeline evaluations. This method leads to a more targeted search, crucial in situations with limited computational resources. Two variants of this approach are proposed and compared with standard TPOT across six datasets, with up to 20 features and 20,000 samples. The results show the proposed method is competitive with canonical TPOT, and outperforms it in some cases. The study also provides new insights into the dynamics of pipeline structure and hyperparameter search within TPOT.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {340–348},
numpages = {9},
location = {Melbourne, VIC, Australia},
series = {GECCO '24}
}

@Inbook{Olson2019,
author="Olson, Randal S.
and Moore, Jason H.",
editor="Hutter, Frank
and Kotthoff, Lars
and Vanschoren, Joaquin",
title="TPOT: A Tree-Based Pipeline Optimization Tool for Automating Machine Learning",
bookTitle="Automated Machine Learning: Methods, Systems, Challenges",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="151--160",
abstract="As data science becomes increasingly mainstream, there will be an ever-growing demand for data science tools that are more accessible, flexible, and scalable. In response to this demand, automated machine learning (AutoML) researchers have begun building systems that automate the process of designing and optimizing machine learning pipelines. In this chapter we present TPOT v0.3, an open source genetic programming-based AutoML system that optimizes a series of feature preprocessors and machine learning models with the goal of maximizing classification accuracy on a supervised classification task. We benchmark TPOT on a series of 150 supervised classification tasks and find that it significantly outperforms a basic machine learning analysis in 21 of them, while experiencing minimal degradation in accuracy on 4 of the benchmarks---all without any domain knowledge nor human input. As such, genetic programming-based AutoML systems show considerable promise in the AutoML domain.",
isbn="978-3-030-05318-5",
doi="10.1007/978-3-030-05318-5_8",
url="https://doi.org/10.1007/978-3-030-05318-5\_8"
}

@article{Romano2021,
  author = {Joseph D. Romano and Trang T. Le and Weixuan Fu and Jason H. Moore},
  title = {TPOT-NN: augmenting tree-based automated machine learning with neural network estimators},
  journal = {Genetic Programming and Evolvable Machines},
  volume = {22},
  number = {2},
  pages = {207--227},
  year = {2021},
  month = {June},
  doi = {10.1007/s10710-021-09401-z},
  url = {https://doi.org/10.1007/s10710-021-09401-z},
  issn = {1573-7632},
  publisher = {Springer}
}

@article{SCHMITT2023200188,
title = {Automated machine learning: AI-driven decision making in business analytics},
journal = {Intelligent Systems with Applications},
volume = {18},
pages = {200188},
year = {2023},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2023.200188},
url = {https://www.sciencedirect.com/science/article/pii/S2667305323000133},
author = {Marc Schmitt},
keywords = {Artificial intelligence, Machine learning, AutoML, Business analytics, Data-driven decision making, Digital transformation, Human empowerment},
abstract = {The realization that AI-driven decision-making is indispensable in today's fast-paced and ultra-competitive marketplace has raised interest in industrial machine learning (ML) applications significantly. The current demand for analytics experts vastly exceeds the supply. One solution to this problem is to increase the user-friendliness of ML frameworks to make them more accessible for the non-expert. Automated machine learning (AutoML) is an attempt to solve the problem of expertise by providing fully automated off-the-shelf solutions for model choice and hyperparameter tuning. This paper analyzed the potential of AutoML for applications within business analytics, which could help to increase the adoption rate of ML across all industries. The H2O AutoML framework was benchmarked against a manually tuned stacked ML model on three real-world datasets. The manually tuned ML model could reach a performance advantage in all three case studies used in the experiment. Nevertheless, the H2O AutoML package proved to be quite potent. It is fast, easy to use, and delivers reliable results, which come close to a professionally tuned ML model. The H2O AutoML framework in its current capacity is a valuable tool to support fast prototyping with the potential to shorten development and deployment cycles. It can also bridge the existing gap between supply and demand for ML experts and is a big step towards automated decisions in business analytics. Finally, AutoML has the potential to foster human empowerment in a world that is rapidly becoming more automated and digital.}
}
}

@article{ALAM2023100341,
title = {An investigation of the imputation techniques for missing values in ordinal data enhancing clustering and classification analysis validity},
journal = {Decision Analytics Journal},
volume = {9},
pages = {100341},
year = {2023},
issn = {2772-6622},
doi = {https://doi.org/10.1016/j.dajour.2023.100341},
url = {https://www.sciencedirect.com/science/article/pii/S2772662223001819},
author = {Shafiq Alam and Muhammad Sohaib Ayub and Sakshi Arora and Muhammad Asad Khan},
keywords = {Classification, Clustering, Imputation, Ordinal data, Partitioning Around Medoids, Multilayer Perceptron},
abstract = {Missing data can significantly impact dataset integrity and suitability, leading to unreliable statistical results, distortions, and poor decisions. The presence of missing values in data introduces inaccuracies in clustering and classification and compromises the reliability and validity of such analyses. This study investigates multiple imputation techniques specifically designed for handling missing values in ordinal data commonly encountered in surveys and questionnaires. Quantitative approaches are used to evaluate different imputation methods on various datasets with varying missing value percentages. By comparing the performance of imputation techniques using clustering metrics and algorithms (e.g., k-means, Partitioning Around Medoids), the study provides valuable insights for selecting appropriate imputation methods for accurate data analysis. Furthermore, the study examines the impact of imputed values on classification algorithms, including k-Nearest Neighbors (kNN), Naive Bayes (NB), and Multilayer Perceptron (MLP). Results demonstrate that the decision tree method is the most effective approach, closely aligning with the original data and achieving high accuracy. In contrast, random number imputation performs poorly, indicating limited reliability. This study advances the understanding of handling missing values and emphasizes the need to address this issue to enhance data analysis integrity and validity.}
}

@INPROCEEDINGS{9338346,
  author={Poulakis, Yannis and Doulkeridis, Christos and Kyriazis, Dimosthenis},
  booktitle={2020 IEEE International Conference on Data Mining (ICDM)}, 
  title={AutoClust: A Framework for Automated Clustering Based on Cluster Validity Indices}, 
  year={2020},
  volume={},
  number={},
  pages={1220-1225},
  keywords={Clustering algorithms;Machine learning;Tools;Data mining;Tuning;Optimization;Unsupervised learning;automatic clustering;hyperparameter tuning;meta-learning},
  doi={10.1109/ICDM50108.2020.00153}}

@INPROCEEDINGS{9671542,
  author={ElShawi, Radwa and Lekunze, Hudson and Sakr, Sherif},
  booktitle={2021 IEEE International Conference on Big Data (Big Data)}, 
  title={cSmartML: A Meta Learning-Based Framework for Automated Selection and Hyperparameter Tuning for Clustering}, 
  year={2021},
  volume={},
  number={},
  pages={1119-1126},
  keywords={Machine learning algorithms;Supervised learning;Clustering algorithms;Machine learning;Big Data;Linear programming;Classification algorithms;clustering;meta-learning;hyper-parameter optimization},
  doi={10.1109/BigData52589.2021.9671542}}

@INPROCEEDINGS{10031201,
  author={ElShawi, Radwa and Sakr, Sherif},
  booktitle={2022 IEEE International Conference on Data Mining Workshops (ICDMW)}, 
  title={cSmartML-Glassbox: Increasing Transparency and Controllability in Automated Clustering}, 
  year={2022},
  volume={},
  number={},
  pages={47-54},
  keywords={Visualization;Machine learning algorithms;Machine learning;Aerospace electronics;Controllability;Usability;Optimization;clustering;meta-learning;hyperparameter optimization},
  doi={10.1109/ICDMW58026.2022.00015}}

@InProceedings{10.1007/978-3-030-75768-720,
author="Liu, Yue
and Li, Shuang
and Tian, Wenjie",
editor="Karlapalem, Kamal
and Cheng, Hong
and Ramakrishnan, Naren
and Agrawal, R. K.
and Reddy, P. Krishna
and Srivastava, Jaideep
and Chakraborty, Tanmoy",
title="AutoCluster: Meta-learning Based Ensemble Method for Automated Unsupervised Clustering",
booktitle="Advances in Knowledge Discovery and Data Mining",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="246--258",
abstract="Automated clustering automatically builds appropriate clustering models. The existing automated clustering methods are widely based on meta-learning. However, it still faces specific challenges: lacking comprehensive meta-features for meta-learning and general clustering validation index (CVI) as objective function. Therefore, we propose a novel automated clustering method named AutoCluster to address these problems, which is mainly composed of Clustering-oriented Meta-feature Extraction (CME) and Multi-CVIs Clustering Ensemble Construction (MC{\$}{\$}^2{\$}{\$}2EC). CME captures the meta-features from spatial randomness and different learning properties of clustering algorithms to enhance meta-learning. MC{\$}{\$}^2{\$}{\$}2EC develops a collaborative mechanism based on clustering ensemble to balance the measuring criterion of different CVIs and construct more appropriate clustering model for given datasets. Extensive experiments are conducted on 150 datasets from OpenML to create meta-data and 33 test datasets from three clustering benchmarks to validate the superiority of AutoCluster. The results show the superiority of AutoCluster for building an appropriate clustering model compared with classical clustering algorithms and CASH method.",
isbn="978-3-030-75768-7"
}

@INPROCEEDINGS{10031132,
  author={ElShawi, Radwa and Sakr, Sherif},
  booktitle={2022 IEEE International Conference on Data Mining Workshops (ICDMW)}, 
  title={TPE-AutoClust: A Tree-based Pipline Ensemble Framework for Automated Clustering}, 
  year={2022},
  volume={},
  number={},
  pages={1144-1153},
  keywords={Scalability;Pipelines;Buildings;Supervised learning;Clustering algorithms;Machine learning;Complexity theory;clustering;meta-learning;hyperparameter opti-mization;evolutionary algorithms},
  doi={10.1109/ICDMW58026.2022.00149}}

@inproceedings{10.1145/3292500.3330701,
author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
title = {Optuna: A Next-generation Hyperparameter Optimization Framework},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330701},
doi = {10.1145/3292500.3330701},
abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {2623–2631},
numpages = {9},
keywords = {machine learning system, hyperparameter optimization, black-box optimization, Bayesian optimization},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@article{10.5555/1953048.2078195,
author = {Pedregosa, Fabian and Varoquaux, Ga\"{e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, \'{E}douard},
title = {Scikit-learn: Machine Learning in Python},
year = {2011},
issue_date = {2/1/2011},
publisher = {JMLR.org},
volume = {12},
number = {null},
issn = {1532-4435},
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
journal = {J. Mach. Learn. Res.},
month = nov,
pages = {2825–2830},
numpages = {6}
}

@ARTICLE{4766909,
  author={Davies, David L. and Bouldin, Donald W.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A Cluster Separation Measure}, 
  year={1979},
  volume={PAMI-1},
  number={2},
  pages={224-227},
  keywords={Dispersion;Density measurement;Algorithm design and analysis;Clustering algorithms;Partitioning algorithms;Multidimensional systems;Data analysis;Performance analysis;Humans;Missiles;Cluster;data partitions;multidimensional data analysis;parametric clustering;partitions;similarity measure},
  doi={10.1109/TPAMI.1979.4766909}}

@article{ROUSSEEUW198753,
title = {Silhouettes: A graphical aid to the interpretation and validation of cluster analysis},
journal = {Journal of Computational and Applied Mathematics},
volume = {20},
pages = {53-65},
year = {1987},
issn = {0377-0427},
doi = {https://doi.org/10.1016/0377-0427(87)90125-7},
url = {https://www.sciencedirect.com/science/article/pii/0377042787901257},
author = {Peter J. Rousseeuw},
keywords = {Graphical display, cluster analysis, clustering validity, classification},
abstract = {A new graphical display is proposed for partitioning techniques. Each cluster is represented by a so-called silhouette, which is based on the comparison of its tightness and separation. This silhouette shows which objects lie well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The average silhouette width provides an evaluation of clustering validity, and might be used to select an ‘appropriate’ number of clusters.}
}

@inproceedings{10.5555/3016100.3016103,
author = {Bachem, Olivier and Lucic, Mario and Hassani, S. Hamed and Krause, Andreas},
title = {Approximate k-means++ in sublinear time},
year = {2016},
publisher = {AAAI Press},
abstract = {The quality of K-Means clustering is extremely sensitive to proper initialization. The classic remedy is to apply k-means++ to obtain an initial set of centers that is provably competitive with the optimal solution. Unfortunately, k-means++ requires k full passes over the data which limits its applicability to massive datasets. We address this problem by proposing a simple and efficient seeding algorithm for K-Means clustering. The main idea is to replace the exact D2-sampling step in k-means++ with a substantially faster approximation based on Markov Chain Monte Carlo sampling. We prove that, under natural assumptions on the data, the proposed algorithm retains the full theoretical guarantees of k-means++ while its computational complexity is only sublinear in the number of data points. For such datasets, one can thus obtain a provably good clustering in sublinear time. Extensive experiments confirm that the proposed method is competitive with k-means++ on a variety of real-world, large-scale datasets while offering a reduction in runtime of several orders of magnitude.},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {1459–1467},
numpages = {9},
location = {Phoenix, Arizona},
series = {AAAI'16}
}

@article{10.1109/TKDE.2022.3155450,
author = {Nie, Feiping and Li, Ziheng and Wang, Rong and Li, Xuelong},
title = {An Effective and Efficient Algorithm for K-Means Clustering With New Formulation},
year = {2023},
issue_date = {April 2023},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {35},
number = {4},
issn = {1041-4347},
url = {https://doi.org/10.1109/TKDE.2022.3155450},
doi = {10.1109/TKDE.2022.3155450},
abstract = {K-means is one of the most simple and popular clustering algorithms, which implemented as a standard clustering method in most of machine learning researches. The goal of K-means clustering is finding a set of cluster centers and minimizing the sum of squared distances between each sample and its nearest clustering center. In this paper, we proposed a novel K-means clustering algorithm, which reformulate the classical K-Means objective function as a trace maximization problem and then replace it with a new formulation. The proposed algorithm does not need to calculate the cluster centers in each iteration and requires fewer additional intermediate variables during the optimization process. In addition, we proposed an efficient iterative re-weighted algorithm to solve the involved optimization problem and provided the corresponding convergence analysis. The proposed algorithm keeps a consistent computational complexity as Lloyd's algorithm, <inline-formula><tex-math notation="LaTeX">$mathcal {O}(ndk)$</tex-math><alternatives><mml:math><mml:mrow><mml:mi mathvariant="script">O</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="li-ieq1-3155450.gif"/></alternatives></inline-formula>, but shows a faster convergence rate in experiments. Extensive experimental results on real world benchmark datasets show the effectiveness and efficiency of the proposed algorithm.},
journal = {IEEE Trans. on Knowl. and Data Eng.},
month = apr,
pages = {3433–3443},
numpages = {11}
}
